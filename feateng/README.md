Feature Engineering
=

Before, you built a logistic regression system from scratch and tested it on
how well it could predict if an answer was correct.

In this homework, you're going to continue to improve the accuracy of such a system by creating
new features.

You will improve the classification by extracting useful information from
the guesses and generate better features for input into the *logistic
regression* classifier to do a better job of selecting whether a guess to a
question is correct.

In the code, a guess is generated by a "Guesser".  This is the
foundation of what you'll put into your classifier.  You need to decide whether to "buzz in" on that guess.  This decision is made by your classifer, the "Buzzer".  In a perfect world, every time your guess was correct, you'd buzz in and every time your guess was wrong you would not.  The initial code here is not perfect, so you're going to use feature engineering to improve that.

NOTE: Because the goal of this assignment is feature engineering, not
classification algorithms, you may not change the underlying algorithm. You
can change add to the guessed answers (e.g., to create a new feature), but you
may not swap out the class that's generating classes nor can you change the
classifier.

This assignment is structured in a way that approximates how classification
works in the real world: features are typically underspecified (or not
specified at all). You have to articulate the features you
need. You then compete against others to provide useful predictions.

It may seem straightforward, but do not start this at the last minute. There
are often many things that go wrong in testing out features, and you'll want
to make sure your features work well once you've found them.

Likewise, because this homework is not going to tell you exactly what
you need to do, you'll need to have understood many of the concepts
we've covered in the class.  The bare essentials are classifiers,
information retrieval, and feature engineering, but it may be helpful
to review our coverage of syntax, semantics, etc.

Getting Started
-

As usual, install the packages you need, perhaps in a virtual environment:

    python3 -m venv .venv
    .venv/bin/pip3 install -r requirements.txt

And if NLTK complains about missing stopwords, you can download them:

    python -m nltk.downloader stopwords

You'll also need to create a directory for the models you'll be
creating

     mkdir -p models
         
But before you get started, you need to understand the overall structure of the code:
 * A part of the question comes into the guesser (in the code, this is called a "run")
 * The guesser generates a "guess" that _could_ be an answer to the question
 * The buzzer then needs to determine if that guess is correct or not.  This is a classifier.  You're going to make that better by providing the buzzer with new features.
 
You will need to be creative here!  To get a sense of how you might go through the process, review the lecture on feature engineering here:
https://www.youtube.com/watch?v=IzKFgigocAg

How to add a feature?
-

First, get an idea of what you want to do.  After training the classifier,
look at your predictions on the dev set and see where they're going wrong.

1.  To add a feature, you need to create a new subclass of the Feature class
in ``features.py``.  This is important so that you can try out different
features by turning them on and off with the command line.

2.  Add code to instantiate the feature in ``params.py``.

3.  (Optionally) Change the API to feed in more information into the feature
generation process.  This would be necessary to capture temporal dynamics or
use, say, information from Wikipedia:
https://drive.google.com/file/d/1-AhjvqsoZ01gz7EMt5VmlCnVpsE96A5n/view?usp=sharing

To walk you through the process, let's create a new feature that encodes how
often the guess appeared in the training set.  The first step is to define the
class in ``features.py``.

	class FrequencyFeature:                       
	    def __init__(self, name):                 
		from eval import normalize_answer   
		self.name = name                      
		self.counts = Counter()               
		self.normalize = normalize_answer     

	    def add_training(self, question_source):    
		import json                 
		with open(question_source) as infile:                   
			questions = json.load(infile)                       
			for ii in questions:                                
			    self.counts[self.normalize(ii["page"])] += 1    

	    def __call__(self, question, run, guess):                               
		   yield ("guess", log(1 + self.counts[self.normalize(guess)]))          


Pay attention to the ``call`` function.  If you're not familiar with
the ``yield`` keyword:
https://realpython.com/introduction-to-python-generators/

One very easy way of adding features is to just yield something else
in a function that you already have.

These will be sent to a DictVectorizer, the first element of the tuple
is the feature name, the second element of the tuple is the feature
value (look at the ``featurize`` function in buzzer.py).

Now that you have a feature class, it needs to be loaded when you run
your code.  This happens in ``params.py``.  Now you can
add the feature name to the command line to turn it on.

    for ff in flags.features:
        if ff == "Length":
            from features import LengthFeature
            feature = LengthFeature(ff)
            buzzer.add_feature(feature)

        if ff == "Frequency":                                  
            from features import FrequencyFeature              
            feature = FrequencyFeature(ff)                     
            feature.add_training("../data/qanta.buzztrain.json")
            buzzer.add_feature(feature)                        

Don't forget that you're training a classifier.  This classifier will
be turned into a "pickle" file and stored in the models directory.  So
let's train the classifier *without* that new feature.

    mkdir -p models
    python3 buzzer.py --guesser_type=Gpr --limit=50 \
      --question_source=json --GprGuesser_filename=../models/gpt_cache \
      --questions=../data/qanta.buzztrain.json --buzzer_guessers Gpr \
      --LogisticBuzzer_filename=models/no_length --features ""
    Setting up logging
    INFO:root:Using device 'cuda' (cuda flag=False)
    INFO:root:Initializing guesser of type Gpr
    INFO:root:Loading Gpr guesser
    Loading buzzer
    INFO:root:Buzzer using run length 100
    INFO:root:Using device 'cuda' (cuda flag=False)
    INFO:root:Initializing guesser of type Gpr
    INFO:root:Loading Gpr guesser
    INFO:root:9310 entries added to cache
    INFO:root:9310 entries added to cache
    INFO:root:Adding Gpr to Buzzer (total guessers=1)
    Initializing features: ['']
    dataset: ../data/qanta.buzztrain.json
    ERROR:root:1 features on command line (['']), but only added 0 (set()).  Did you add code to params.py's load_buzzer to actually add the feature to the buzzer?  Or did you forget to increment features_added in that function?
    INFO:root:Read 50 questions
    100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 118416.26it/s]
    INFO:root:Building guesses from dict_keys(['Gpr'])
    INFO:root:Generating guesses for 401 new question
    100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [00:00<00:00, 501166.84it/s]
    INFO:root:       401 guesses from Gpr
    INFO:root:Generating all features
    100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [00:00<00:00, 54978.95it/s]
    Ran on 50 questions of 50

If you get a warning about convergence, it is okay; hopefully it will converge better with more features!  Likewise, don't worry about the warning about the features, I just wanted to be sure it didn't add the length feature.  Because we want to do that next: train a model *with* that new feature.  Note that we're naming the model something different:

    python3 buzzer.py --guesser_type=Gpr --limit=50 \
      --question_source=json --GprGuesser_filename=../models/gpt_cache \
      --questions=../data/qanta.buzztrain.json --buzzer_guessers Gpr \
      --LogisticBuzzer_filename=models/with_length --features Length
    Setting up logging
    INFO:root:Using device 'cuda' (cuda flag=False)
    INFO:root:Initializing guesser of type Gpr
    INFO:root:Loading Gpr guesser
    Loading buzzer
    INFO:root:Buzzer using run length 100
    INFO:root:Using device 'cuda' (cuda flag=False)
    INFO:root:Initializing guesser of type Gpr
    INFO:root:Loading Gpr guesser
    INFO:root:9310 entries added to cache
    INFO:root:9310 entries added to cache
    INFO:root:Adding Gpr to Buzzer (total guessers=1)
    Initializing features: ['Length']
    dataset: ../data/qanta.buzztrain.json
    INFO:root:Adding feature Length
    INFO:root:Read 50 questions
    100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 118751.53it/s]
    INFO:root:Building guesses from dict_keys(['Gpr'])
    INFO:root:Generating guesses for 401 new question
    100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [00:00<00:00, 499084.84it/s]
    INFO:root:       401 guesses from Gpr
    INFO:root:Generating all features
    100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [00:00<00:00, 46940.24it/s]
    Ran on 50 questions of 50
    

Now you need to evaluate the classifier.  The script eval.py will run the classifier on all of your data and then record the outcome.  There are several things that could happen:
 * _best_: Guess was correct, Buzz was correct
 * _timid_: Guess was correct, Buzz was not
 * _aggressive_: Guess was wrong, Buzz was wrong
 * _waiting_: Guess was wrong, Buzz was correct


Let's compare with the Length: 

    python3 buzzer.py --guesser_type=GprGuesser --limit=50 \
    --question_source=json --GprGuesser_filename=../models/GprGuesser \
    --questions=../data/qanta.buzztrain.json --buzzer_guessers GprGuesser \
    --features Length Frequency

compared to without it:

    .venv/bin/python3  eval.py --guesser_type=Gpr \
    --TfidfGuesser_filename=models/TfidfGuesser --limit=25 \
     --questions=../data/qanta.buzzdev.json.gz --buzzer_guessers Gpr \
     --GprGuesser_filename=../models/gpt_cache  \
     --LogisticBuzzer_filename=models/no_length --features ""

You'll see quite a bit of output, so I'm just going to walk through it bit by
    bit, comparing the salient components.

 Now, both "best" and "waiting" are *correct*, but obviously "best" is best.  It's important to know what kind of examples contribute to each of these outcomes, so eval samples a subset for each of these and prints them and their features out.

    =================
    aggressive 0.22
    ===================
    
                   guess: The Awakening (Chopin novel)
                  answer: Edna_Pontellier
                      id: 93160
          Gpr_confidence: -0.1257
             Length_char: -0.1111
             Length_word: -0.1333
            Length_guess: 3.3673
                    text: This character faintheartedly commits herself to improving her studies
                          after a night of reading Emerson alone in her house, and hushes Victor
                          when he begins singing "Ah! Si tu savais!" While talking to a friend,
                          she declares that she would give up the "unessential things" for her
                          children, but she wouldn't give herself up. Doctor Mandelet advises
                          this character's husband to permit her whims, which

This example is where it is answering the name of the novel rather than the book's main character.  You can see all of the features for this example (e.g., Length-guess is 3.3673).

At the end of the eval script, you can see the
overall accuracy, and the ratio of correct buzzes to incorrect buzzes (should
be positive), and the buzz position (where in the question it's buzzing).

    Questions Right: 90 (out of 201) Accuracy: 0.75  Buzz ratio: 67.50 Buzz position: 0.054159

And now we'll see what it is without the length features:

    Questions Right: 87 (out of 201) Accuracy: 0.76  Buzz ratio: 66.50 Buzz position: -0.255239

Again, don't focus too much on the accuracy.  The accuracy is actually higher
for the no feature model!  But the proportion of "Best" outcomes is higher by
0.02 once you add in this simple feature.

At the very end of the script, you see the weights of each of the features.  Higher values mean that when the feature is high, it is more likely to buzz.  Lower features mean that when the feature is high, it is less likely to buzz.  Features near zero are ignored.  However, keep in mind the average value of the feature ... I'd encourage you to keep your features with mean zero and standard variance to make your life easier.

Let's see with length:
                          Gpr_confidence: 4.4401
                             Length_char: 0.9581
                            Length_guess: -1.0036
                             Length_word: 0.8495
And without length:
                          Gpr_confidence: 5.5703

The classifier with the length is more liketly to buzz later in the question.  If you only have the guesser confidence, then it's obviously correlated with that.  It uses it less if you add in the length as a feature.

What Can You Do?
-

You can:
* Add features (e.g., to params.py)
* Change feature representations (e.g., features.py)
* Exclude data 
* Add data

Good Enough
-
This is a very open-ended assignment.  Improve the "best" class by
at least 0.02 percent by adding new features, and you have done enough.  

What Can't You Do?
-
Change the static guesses or use a different classifier (buzzer in this lingo).

How to start
-
1. Remind yourself how to run the sklearn logistic regression (logistic_buzzer.py)
2. Add a simple feature to the training data generated by gpr_guesser.py 
3. See if it increases the accuracy on held-out data when you run logistic regression (eval.py) or on the leaderboard
4. Rinse and repeat!




Accuracy (15+ points)
------------------------------

15 points of your score will be generated from your performance on the
the classification competition on the leaderboard.  The performance will be
evaluated on accuracy on a held-out test set.

You should be able to significantly
improve on the baseline system.  If you can
do much better than your peers, you can earn extra credit (up to 10 points).

Analysis (10 Points)
--------------

The job of the written portion of the homework is to convince the grader that:
* Your new features work
* You understand what the new features are doing
* You had a clear methodology for incorporating the new features

Make sure that you have examples and quantitative evidence that your
features are working well, and include the metrics you chose to measure your system's performance. Be sure to explain how used the data
(e.g., did you have a development set?) and how you inspected the
results.

A sure way of getting a low grade is simply listing what you tried and
reporting the corresponding metrics for each attempt.  You are expected to pay more
attention to what is going on with the data and take a data-driven
approach to feature engineering.

How to Turn in Your System
-
* ``features.py``: This file includes an implementation of your new features.
* ``params.py``: This instantiates your new features.  Modify this so that the
set of your best features runs by *default*.  
* **Custom Training Data** (If you used additional training data beyond the Wikipedia pages, upload that as well
    * (OR) If either any of your files are >100MB, please submit a shell
    script named ``gather_resources.sh`` that will retrieve one or both of the
    files above programatically from a public location (i.e a public S3
    bucket).
* The LogisticBuzzer.model.pkl file and LogisticBuzzer.featurizer.pkl file created by training the classifier.
* ``analysis.pdf``: Your **PDF** file containing your feature engineering
analysis.

Turn in the above files as usual via Gradescope, where we'll be using the
leaderboard as before.  However, the position on the leaderboard will count
for more of your grade.

FAQ
-----------------

*Q.* Eval only shows me what the questions I'm getting right and wrong
are.  How do I know what the features look like?

*A.* Use ``features.py`` to investigate this.  This is how we
generated the JSON files for the logistic regression homework. 

    python3 features.py --json_guess_output=../data/inspect.jsonl --buzzer_guessers 'Gpr' --questions=../data/qanta.buzztrain.json.gz --limit=1000

Make sure that you've enabled all of the features that you want to use.

*Q.* Why can't I use ``['page']`` or ``['answer']`` when creating
features?  Can I use it during training?

*A.* You cannot use those fields nor "page" / "text" when generating features,
however, as that would be cheating.  That's why they get removed
before the feature generator is called.  If you need the current text
available, that's the "run", and your job is to see if the current
"guess" is correct or not.

Now, that's not to say that you can never use the page field.  During
training, you can of course use the text and page.  You can see this
in the example Frequency feature: it uses the page to compute how
often each correct response is.  You then check for a *guess* that comes
in how frequent it is.

But that's the exception, usually the only way you would use the real
'page' during training is as the label to the classifier: is this
guess correct becomes a positive example, is this guess incorrect
becomes a negative example.

*Q:* Can I modify buzzer.py so that I can use the history of guesses in a
 question?

*A:* Yes.  If you do that, make sure to upload buzzer.py.  We will replace the
 default version of buzzer.py with your new submission.

*Q:* Can I use the <INSERT NAME HERE> package?

*A:* Clear it first on Piazza.  We'll provide spacy and nltk for sure (along
 with all of the packages already used in this homework).  We
 won't allow packages that require internet access (e.g., wikipedia).  We
 don't have anything against Wikipedia (we provide this json file so you can
 use it), but we don't want to get our IP
 address banned.

*Q:* Sometimes the guess is correct but it isn't counted that way.  And
 sometimes a wrong answer is counted as correct.

*A:* Yes, and we'll cover this in more detail later in the course.  For now,
 this is something we'll have to live with.

*Q:* What is the guesser that we're using?  Where are the guesses
coming from?

*A:* These are cached guesses from OpenAI's GPT.  We'll get into
 generating our own guesses in the next homework.  You probably will
 want to play around a little bit with the output of its guesses, as
 there's likely interesting stuff you can use from there. Let's play
 around with the GprGuesser a little:
 
 
 
The keys are the prompts to GPT.  We've given GPT the closest examples
we can find in Wikipedia and from our existing dataset.  


Let's take a look at what this returned.  We get the title, but that's
not all.  Remember that GPT is just a language model, so it generates
one word piece at a time, and we have a probability for each word
piece.

You may want to use these to create features!

*Q:* What if I get the error that ``GprGuesser`` has no attribute 'predict'?

*A:* This means that you're running it on a guesser result that hasn't been
 cached or that it can't find the cache file.  Make sure the path is correct,
 and use the limit option to only process a handful of examples.
